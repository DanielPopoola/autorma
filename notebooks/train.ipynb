{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdc9f28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1305c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93580587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import timm\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATA_DIR = Path(\"/content/drive/MyDrive/data/processed\")\n",
    "OUTPUT_DIR = Path(\"/content/drive/MyDrive/models\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 15\n",
    "LEARNING_RATE = 1e-4\n",
    "IMG_SIZE = 224\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ea02bc",
   "metadata": {},
   "source": [
    "## Load dataset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a66aa993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['shirts', 'watches', 'casual_shoes', 'tops', 'handbags']\n"
     ]
    }
   ],
   "source": [
    "with open(DATA_DIR / \"dataset_info.json\") as f:\n",
    "    dataset_info = json.load(f)\n",
    "\n",
    "NUM_CLASSES = len(dataset_info[\"categories\"])\n",
    "CLASS_NAMES = [c.lower().replace(\" \", \"_\") for c in dataset_info[\"categories\"]]\n",
    "print(f\"Classes: {CLASS_NAMES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede2f548",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73807e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1750, Val: 375, Test: 375\n",
      "Class mapping: {'casual shoes': 0, 'handbags': 1, 'shirts': 2, 'tops': 3, 'watches': 4}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = datasets.ImageFolder(DATA_DIR / \"train\", transform=train_transform)\n",
    "val_dataset = datasets.ImageFolder(DATA_DIR / \"val\", transform=val_transform)\n",
    "test_dataset = datasets.ImageFolder(DATA_DIR / \"test\", transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\"\n",
    ")\n",
    "print(f\"Class mapping: {train_dataset.class_to_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80a7dbd",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11714522",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model(\"efficientnet_b0\", pretrained=True, num_classes=NUM_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"max\", patience=2, factor=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eeb28f",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e740a2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss, train_correct, train_total = 0, 0, 0\n",
    "\n",
    "    for images, labels in tqdm(\n",
    "        train_loader, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS} [Train]\"\n",
    "    ):\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    train_acc = 100.0 * train_correct / train_total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(\n",
    "            val_loader, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS} [Val]\"\n",
    "        ):\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    val_acc = 100.0 * val_correct / val_total\n",
    "\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.2f}%, Val Loss={val_loss:.4f}, Val Acc={val_acc:.2f}%\"\n",
    "    )\n",
    "\n",
    "    scheduler.step(val_acc)\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"val_acc\": val_acc,\n",
    "                \"class_to_idx\": train_dataset.class_to_idx,\n",
    "            },\n",
    "            OUTPUT_DIR / \"best_model.pth\",\n",
    "        )\n",
    "        print(f\"✓ Saved best model (Val Acc: {val_acc:.2f}%)\")\n",
    "\n",
    "print(f\"\\nBest validation accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c658f3",
   "metadata": {},
   "source": [
    "## Test evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997175d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(OUTPUT_DIR / \"best_model.pth\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "test_correct, test_total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "test_acc = 100.0 * test_correct / test_total\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b06723",
   "metadata": {},
   "source": [
    "## Save training metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a73513",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    \"model_architecture\": \"efficientnet_b0\",\n",
    "    \"num_classes\": NUM_CLASSES,\n",
    "    \"class_names\": CLASS_NAMES,\n",
    "    \"class_to_idx\": train_dataset.class_to_idx,\n",
    "    \"img_size\": IMG_SIZE,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"num_epochs\": NUM_EPOCHS,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"best_val_acc\": best_val_acc,\n",
    "    \"test_acc\": test_acc,\n",
    "    \"history\": history,\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / \"training_metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"✓ Training complete. Files saved to:\", OUTPUT_DIR)\n",
    "print(\"  - best_model.pth\")\n",
    "print(\"  - training_metadata.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f956c3",
   "metadata": {},
   "source": [
    "## Plot training curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d465500",
   "metadata": {},
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(history['train_loss'], label='Train Loss')\n",
    "ax1.plot(history['val_loss'], label='Val Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "\n",
    "ax2.plot(history['train_acc'], label='Train Acc')\n",
    "ax2.plot(history['val_acc'], label='Val Acc')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.legend()\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
