{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e615b4cd",
   "metadata": {},
   "source": [
    "##  Prepare data in expected format for training(to be run in kaggle notebooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55caf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "dataset_path = Path('/kaggle/input/fashion-product-images-dataset/fashion-dataset')\n",
    "\n",
    "# Categories to keep\n",
    "TARGET_CATEGORIES = ['Shirts', 'Watches', 'Casual Shoes', 'Tops', 'Handbags']\n",
    "IMAGES_PER_CLASS = 500\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "# Output directory\n",
    "output_dir = Path('/kaggle/working/fashion_subset')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "print(\"Exploring dataset structure...\")\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    print(f\"{root}: {len(files)} files, {len(dirs)} dirs\")\n",
    "    if len(files) > 0:\n",
    "        print(f\"Sample files: {files[:3]}\")\n",
    "    break\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "styles_path = dataset_path / 'styles.csv'\n",
    "if styles_path.exists():\n",
    "    df = pd.read_csv(styles_path, on_bad_lines='skip')\n",
    "    print(f\"\\nDataset columns: {df.columns.tolist()}\")\n",
    "    print(f\"Total images: {len(df)}\")\n",
    "    print(f\"\\nCategory distribution:\\n{df['articleType'].value_counts().head(10)}\")\n",
    "    \n",
    "\n",
    "    df_filtered = df[df['articleType'].isin(TARGET_CATEGORIES)]\n",
    "    print(f\"\\nFiltered to {len(df_filtered)} images across {TARGET_CATEGORIES}\")\n",
    "    print(f\"Distribution: {df_filtered['articleType'].value_counts()}\")\n",
    "    \n",
    "    sampled_images = []\n",
    "    for category in TARGET_CATEGORIES:\n",
    "        cat_df = df_filtered[df_filtered['articleType'] == category]\n",
    "        sample_size = min(IMAGES_PER_CLASS, len(cat_df))\n",
    "        sampled = cat_df.sample(n=sample_size, random_state=42)\n",
    "        sampled_images.append(sampled)\n",
    "        print(f\"{category}: sampled {sample_size} images\")\n",
    "    \n",
    "    df_sampled = pd.concat(sampled_images)\n",
    "    print(f\"\\nTotal sampled: {len(df_sampled)} images\")\n",
    "    \n",
    "    # Split into train/val/test\n",
    "    splits = {'train': [], 'val': [], 'test': []}\n",
    "    \n",
    "    for category in TARGET_CATEGORIES:\n",
    "        cat_images = df_sampled[df_sampled['articleType'] == category]['id'].tolist()\n",
    "        random.seed(42)\n",
    "        random.shuffle(cat_images)\n",
    "        \n",
    "        n_train = int(len(cat_images) * TRAIN_RATIO)\n",
    "        n_val = int(len(cat_images) * VAL_RATIO)\n",
    "        \n",
    "        splits['train'].extend([(img, category) for img in cat_images[:n_train]])\n",
    "        splits['val'].extend([(img, category) for img in cat_images[n_train:n_train+n_val]])\n",
    "        splits['test'].extend([(img, category) for img in cat_images[n_train+n_val:]])\n",
    "    \n",
    "    print(f\"\\nSplit sizes:\")\n",
    "    print(f\"Train: {len(splits['train'])}\")\n",
    "    print(f\"Val: {len(splits['val'])}\")\n",
    "    print(f\"Test: {len(splits['test'])}\")\n",
    "    \n",
    "    # Copy images to organized structure\n",
    "    images_dir = dataset_path / 'images'\n",
    "    \n",
    "    for split_name, images in splits.items():\n",
    "        for img_id, category in images:\n",
    "            src = images_dir / f\"{img_id}.jpg\"\n",
    "            if not src.exists():\n",
    "                continue\n",
    "            \n",
    "            dst_dir = output_dir / split_name / category.lower()\n",
    "            dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "            dst = dst_dir / f\"{img_id}.jpg\"\n",
    "            \n",
    "            shutil.copy2(src, dst)\n",
    "    \n",
    "    # Save dataset info\n",
    "    dataset_info = {\n",
    "        'categories': TARGET_CATEGORIES,\n",
    "        'images_per_class': IMAGES_PER_CLASS,\n",
    "        'splits': {\n",
    "            'train': len(splits['train']),\n",
    "            'val': len(splits['val']),\n",
    "            'test': len(splits['test'])\n",
    "        },\n",
    "        'split_ratios': {\n",
    "            'train': TRAIN_RATIO,\n",
    "            'val': VAL_RATIO,\n",
    "            'test': TEST_RATIO\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(output_dir / 'dataset_info.json', 'w') as f:\n",
    "        json.dump(dataset_info, f, indent=2)\n",
    "    \n",
    "    print(\"\\n✓ Dataset preparation complete!\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    \n",
    "    # Verify structure\n",
    "    print(\"\\nFinal structure:\")\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        split_dir = output_dir / split\n",
    "        if split_dir.exists():\n",
    "            for cat_dir in split_dir.iterdir():\n",
    "                if cat_dir.is_dir():\n",
    "                    count = len(list(cat_dir.glob('*.jpg')))\n",
    "                    print(f\"{split}/{cat_dir.name}: {count} images\")\n",
    "    \n",
    "    # Create zip for download\n",
    "    shutil.make_archive('/kaggle/working/fashion_subset', 'zip', output_dir)\n",
    "    print(\"\\n✓ Created fashion_subset.zip for download\")\n",
    "    print(f\"Size: {os.path.getsize('/kaggle/working/fashion_subset.zip') / 1024 / 1024:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
