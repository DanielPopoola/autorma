# Refund Item Classification System

An end-to-end machine learning system for automated classification of returned items in an e-commerce warehouse. Built with production-grade MLOps practices including model versioning, batch inference pipelines, monitoring, and a user-friendly interface.

## üéØ Project Overview

This system demonstrates a complete ML deployment workflow that goes beyond model training to include:
- Automated batch inference pipeline with checkpoint recovery
- Model versioning and registry with MLflow
- Production monitoring with Prometheus and Grafana
- RESTful model serving with FastAPI
- Interactive UI for manual batch processing
- Scheduled automation via cron jobs

**Key Metrics:**
- Model Accuracy: 96.53% on test set
- Processing Speed: ~5 seconds per batch (10 images)
- Categories: 5 (Casual Shoes, Handbags, Shirts, Tops, Watches)
- Total Dataset: 2,500 images

---

## üèóÔ∏è System Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      User Interfaces                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ  Streamlit UI    ‚îÇ         ‚îÇ  Grafana Dashboard  ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  (Manual Upload) ‚îÇ         ‚îÇ  (Monitoring)       ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ                                ‚îÇ
            ‚ñº                                ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  Orchestration Layer                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  Batch Orchestrator (Python Script)                  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Scans input directory for new images              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Manages checkpoints for recovery                  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Calls Model Service API                           ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Saves results and updates metrics                 ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ                                   ‚îÇ
            ‚îÇ HTTP POST /predict                ‚îÇ Metrics
            ‚ñº                                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Model Service (API)    ‚îÇ      ‚îÇ   Prometheus + Pushgateway‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ      ‚îÇ  - Scrapes /metrics      ‚îÇ
‚îÇ  ‚îÇ  FastAPI Server    ‚îÇ  ‚îÇ      ‚îÇ  - Stores time series    ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îÇ      ‚îÇ  - Feeds Grafana         ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ EfficientNet ‚îÇ  ‚îÇ  ‚îÇ      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ  ‚îÇ  ‚îÇ B0 Model     ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ         ‚ñ≤                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ Load model
          ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   MLflow Registry        ‚îÇ
‚îÇ  - Model versioning      ‚îÇ
‚îÇ  - Experiment tracking   ‚îÇ
‚îÇ  - Production/Staging    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Scheduled Automation:
  Cron (2 AM daily) ‚Üí Batch Orchestrator ‚Üí Process overnight returns
```

See [ARCHITECTURE.md](ARCHITECTURE.md) for detailed system design.

---

## üìÅ Project Structure

```
autorma/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ processed/              # Training/val/test datasets
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ val/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test/
‚îÇ   ‚îî‚îÄ‚îÄ inference/              # Batch processing data
‚îÇ       ‚îú‚îÄ‚îÄ input/              # New images to classify
‚îÇ       ‚îú‚îÄ‚îÄ output/             # Prediction results
‚îÇ       ‚îî‚îÄ‚îÄ checkpoints/        # Recovery checkpoints
‚îÇ
‚îú‚îÄ‚îÄ model-service/              # FastAPI prediction service
‚îÇ   ‚îú‚îÄ‚îÄ app.py
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îÇ
‚îú‚îÄ‚îÄ orchestrator/               # Batch inference pipeline
‚îÇ   ‚îú‚îÄ‚îÄ batch_inference.py
‚îÇ   ‚îú‚îÄ‚îÄ metrics_pusher.py
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îÇ
‚îú‚îÄ‚îÄ streamlit-ui/               # Web interface
‚îÇ   ‚îú‚îÄ‚îÄ app.py
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îÇ
‚îú‚îÄ‚îÄ monitoring/                 # Prometheus + Grafana
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml
‚îÇ   ‚îî‚îÄ‚îÄ prometheus.yml
‚îÇ
‚îú‚îÄ‚îÄ mlflow_data/                # MLflow artifacts and metadata
‚îÇ   ‚îú‚îÄ‚îÄ artifacts/
‚îÇ   ‚îú‚îÄ‚îÄ mlruns/
‚îÇ   ‚îî‚îÄ‚îÄ mlflow.db
‚îÇ
‚îú‚îÄ‚îÄ models/                     # Trained model checkpoints
‚îÇ   ‚îî‚îÄ‚îÄ v1/
‚îÇ       ‚îú‚îÄ‚îÄ best_model.pth
‚îÇ       ‚îî‚îÄ‚îÄ training_metadata.json
‚îÇ
‚îú‚îÄ‚îÄ scripts/                    # Utility scripts
‚îÇ   ‚îú‚îÄ‚îÄ register_model.py
‚îÇ   ‚îî‚îÄ‚îÄ set_production.py
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                  # Training notebooks
‚îÇ   ‚îî‚îÄ‚îÄ 01_data_preparation.ipynb
‚îÇ
‚îú‚îÄ‚îÄ logs/                       # Application logs
‚îÇ
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ ARCHITECTURE.md
‚îî‚îÄ‚îÄ DEVELOPMENT.md
```

---

## üöÄ Quick Start

### Prerequisites

- Python 3.10+
- 8GB RAM minimum
- Docker & Docker Compose (for monitoring)
- WSL2 (if on Windows)

### 1. Clone and Setup

```bash
git clone https://github.com/DanielPopoola/autorma.git
cd autorma

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies for all components
pip install -r model-service/requirements.txt
pip install -r orchestrator/requirements.txt
pip install -r streamlit-ui/requirements.txt
```

### 2. Start MLflow Server

```bash
ABS_PATH=$(pwd)
mlflow server \
  --backend-store-uri sqlite:///$ABS_PATH/mlflow_data/mlflow.db \
  --default-artifact-root file://$ABS_PATH/mlflow_data/artifacts \
  --host 0.0.0.0 \
  --port 5000
```

Access at: http://localhost:5000

### 3. Start Model Service

```bash
# In a new terminal
cd model-service
uvicorn app:app --host 0.0.0.0 --port 8000
```

Test health: `curl http://localhost:8000/health`

### 4. Start Monitoring Stack

```bash
cd monitoring
docker-compose up -d
```

Access:
- Grafana: http://localhost:3000 (admin/admin)
- Prometheus: http://localhost:9090

### 5. Run Streamlit UI

```bash
streamlit run streamlit-ui/app.py
```

Access at: http://localhost:8501

---

## üìä Usage

### Manual Batch Processing (via UI)

1. Open Streamlit UI at http://localhost:8501
2. Navigate to "Upload & Classify" tab
3. Upload images (JPG/PNG)
4. Click "Run Classification"
5. View results in real-time
6. Download results as CSV

### Automated Batch Processing (via CLI)

```bash
# Place images in input directory
cp /path/to/images/* data/inference/input/

# Run batch inference
python orchestrator/batch_inference.py

# View results
cat data/inference/output/predictions_*.json
```

### Scheduled Automation (Cron)

The system runs automatically every night at 2 AM:

```bash
# Edit crontab
crontab -e

# Add this line
0 2 * * * cd /path/to/autorma && source .venv/bin/activate && python orchestrator/batch_inference.py >> logs/cron.log 2>&1
```

---

## üîÑ Model Update Workflow

### Training a New Model

1. Train model on Colab (see notebooks/)
2. Download checkpoint to `models/v{N}/`
3. Register in MLflow:

```bash
python scripts/register_model.py
```

### Promoting to Production

```bash
python scripts/set_production.py
```

This updates the production alias. Model Service will load the new version on next restart.

### Rollback

```python
import mlflow
mlflow.set_tracking_uri("http://127.0.0.1:5000")
client = mlflow.MlflowClient()

# Rollback to version 1
client.set_registered_model_alias("refund-classifier", "production", "1")
```

Restart Model Service to apply.

---

## üìà Monitoring

### Key Metrics Tracked

**Model Service:**
- Request rate and latency (p50, p95, p99)
- Prediction confidence distribution
- Images processed per class
- API success/failure rate

**Batch Orchestrator:**
- Images processed per run
- Batch processing duration
- Success rate
- Failed images count

### Accessing Dashboards

**Grafana:** http://localhost:3000
- Dashboard: "Refund Classifier Monitoring"
- Real-time metrics visualization
- Historical trends

**Prometheus:** http://localhost:9090
- Raw metrics queries
- Target health status

---

## üß™ Testing

### Test Model Service

```bash
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
  -d '{
    "image_paths": ["/absolute/path/to/test/image.jpg"]
  }'
```

### Test Batch Processing

```bash
# Copy test images
find data/processed/test -name "*.jpg" | shuf -n 20 | xargs -I {} cp {} data/inference/input/

# Run orchestrator
python orchestrator/batch_inference.py

# Verify results
ls -lh data/inference/output/
```

### Verify Monitoring

```bash
# Check Prometheus targets
curl http://localhost:9090/api/v1/targets

# Check metrics endpoint
curl http://localhost:8000/metrics | grep images_processed
```

---

## üõ†Ô∏è Troubleshooting

### Model Service won't start

**Issue:** `model_loaded` shows 0 in metrics

**Solution:**
1. Check MLflow is running: `curl http://localhost:5000/health`
2. Verify model is registered: Check MLflow UI
3. Ensure production alias is set: `python scripts/set_production.py`

### Prometheus can't scrape Model Service

**Issue:** Targets show DOWN in Prometheus

**Solution (WSL):**
1. Get WSL IP: `ip addr show eth0 | grep -oP '(?<=inet\s)\d+(\.\d+){3}'`
2. Update `monitoring/prometheus.yml` with your IP
3. Restart: `docker-compose restart prometheus`

### Batch processing fails

**Issue:** Images not being processed

**Solution:**
1. Check Model Service is running: `curl http://localhost:8000/health`
2. Verify image paths are absolute
3. Check logs: `tail -f logs/orchestrator.log`
4. Look for checkpoint issues: `cat data/inference/checkpoints/checkpoint.json`

---

## üìö Documentation

- [ARCHITECTURE.md](docs/ARCHITECTURE.md) - Detailed system design and component breakdown
- [DEVELOPMENT.md](docs/DEVELOPMENT.md) - Development guide and implementation notes

---

## üéì Key Learnings & Design Decisions

### Why Batch Processing?

- **Cost Efficiency:** Process overnight during low-traffic hours
- **Resource Optimization:** Batch GPU inference is more efficient than single predictions
- **Business Alignment:** Returns are processed daily, not real-time
- **Simplicity:** Avoids complexity of real-time streaming systems

### Why Separate Model Service?

- **Testability:** Can test inference independently
- **Deployability:** Update orchestration logic without reloading model
- **Scalability:** Can scale Model Service separately if needed
- **Technology Flexibility:** Could rewrite orchestrator in Go without touching ML code

### Why MLflow?

- **Reproducibility:** Track experiments, hyperparameters, metrics
- **Version Control:** Manage model versions with aliases
- **Easy Rollback:** Quickly revert to previous model if needed
- **Team Collaboration:** Multiple data scientists can share experiments

### Why Prometheus + Grafana?

- **Industry Standard:** Production monitoring pattern
- **Time-Series Data:** Perfect for tracking metrics over time
- **Alerting Ready:** Can add alerts on metric thresholds
- **Visualization:** Grafana provides professional dashboards

---

## üë§ Author

Built as a final year project demonstrating end-to-end ML systems engineering.

**Technologies:** Python, PyTorch, FastAPI, MLflow, Prometheus, Grafana, Streamlit, Docker

---

## üìÑ License

MIT License - See LICENSE file for details
